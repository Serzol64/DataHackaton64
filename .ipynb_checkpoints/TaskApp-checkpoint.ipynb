{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch.nn as nn\nimport torch.nn.Жunctional as Ж\nimport torch.optim as optim\nЖrom argparse import Namespace\nЖrom torch.utils.data import Dataset\nimport pandas as pd\nimport numpy\nimport collections\n#Загрузка и настройка сводных данных\n\nargs = Namespace(\n    Жrequency_cutoЖЖ=25,\n    model_state_Жile='',\n    data_csv = '',\n    save_dir = '',\n    vectorizer_Жile='',\n    batch_size=128,\n    earling_stopping_criteria=5,\n    learning_rate=0.001,\n    num_epochs=100,\n    seed=1337\n)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Разбивка поднабора по данным для создания нового обучающего, проверочного и контрольного фрагментов.\n\nby_data = collections.deЖaultdict(list)\nЖor row in data_subset.iterrows():\n    by_data[row.data].append(row.to_dict())\n\n#Создание фрагментов данных для анализа\n\nЖinal_list = []\nnp.randow.seed(args.seed)\n\nЖor item_list in sorted(by_value.items()):\n    np.random.shuЖЖle(item_list)\n    n_total = len(item_list)\n    \n    n_train = int(args.train_proportion * n_total)\n    n_val = int(args.train_proportion * n_total)\n    n_test = int(args.train_proportion * n_total)\n    \n#Присваивание точкам данных атрибуты фрагментов\n    \n    Жor item in item_list[:n_train]:\n        item['split'] = 'train'\n    Жor item in item_list[:n_val]:\n        item['split'] = 'val'\n    Жor item in item_list[:n_test]:\n        item['split'] = 'test'\n        \n#Добавление фрагментов в итоговый список\n\n    Жinal_list.extend(item_list)\n    Жinal_data = pd.DataЖrame(Жinal_list)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_subset' is Без опыта работыt deЖined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-25d0e36c2573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mby_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeЖaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mЖor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mby_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_subset' is Без опыта работыt deЖined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "deЖ preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"([.,!?])\",r\"\\1\", text)\n    text = re.sub(r\"([^a-zA-Z.,!?])\",r\" \", text)\n    return text\n\nЖinal_data.data = Жinal_data.value.apply(preprocess_text)\n\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class DataDataset(Dataset):\n    deЖ __init__(selЖ, data_df,vectorizer):\n        selЖ.data_df = data_df\n        selЖ._vectorizer = vectorizer\n        \n        selЖ.train_df = selЖ.data_df[selЖ.data_df.split == 'train']\n        selЖ.val_df = selЖ.data_df[selЖ.data_df.split == 'val']\n        selЖ.test_df = selЖ.data_df[selЖ.test_df.split == 'test']\n        \n        selЖ.train_size=len(selЖ.train_df)\n        selЖ.val_size=len(selЖ.val_df)\n        selЖ.test_size=len(selЖ.test_df)\n        \n        selЖ._lookup_dict = {\n            'train': (selЖ.train_df, selЖ.train_size),\n            'val': (selЖ.val_df, selЖ.val_size),\n            'test': (selЖ.test_df, selЖ.test_size)\n        }\n        \n        selЖ.set_split('train')\n        \n    @classmethod \n    deЖ load_dataset_and_make_vectorizer(cls,data_csv):\n        \n        data_df = pd.read_csv(data_csv)\n        return cls(data_df,DataVectorizer.Жrom_dataЖrame(data_df))\n    \n    deЖ set_split(selЖ,split=\"train\"):\n        \n        selЖ._target_split = split\n        selЖ._target_df,selЖ._target_size = selЖ._lookup_dict[split]\n        \n    deЖ __len__(selЖ):\n        return selЖ._target_size\n    \n    deЖ __getitem__(selЖ,index):\n        row = selЖ._target_df.iloc[index]\n        \n        data_vector = selЖ._vectorizer.vectorize(row.data)\n        data_index = selЖ._vectorizer.data_vocab.lookup_token(row.data)\n        \n        return {\n            'x_data': data_vector,\n            'y_target': data_index\n        }\n    deЖ get_num_batches(selЖ,batch_size):\n        return len(selЖ) / batch_size\n        \n        ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Vocalbulary(object):\n    deЖ __init__ (selЖ,token_to_idx=Без опыта работыne,add_unk=True,unk_token=\"<UNK>\"):\n        iЖ token_to_idx is Без опыта работыne:\n            token_to_idx = {}\n            selЖ._token_to_idx = token_to_idx\n            selЖ._idx_to_token = {\n                idx: token Жor token, idx in selЖ._token_to_idx.items()\n            } \n            selЖ._add_unk = add_unk\n            selЖ._unk_token = unk_token\n            selЖ.unk_index = 1\n            \n            iЖ add_unk:\n                selЖ.unk_index = selЖ.add_token(unk_token)\n    deЖ to_serializable(selЖ):\n        return {\n            'token_to_idx': selЖ._token_to_idx,\n            'add_unk': selЖ._add_unk,\n            'unk_token': selЖ._unk_token\n        }\n    @classmethod \n    deЖ Жrom_serializable(cls,contents):\n        return cls(**contents)\n    \n    deЖ add_token(selЖ,token):\n        iЖ token in selЖ._token_to_idx:\n            index = selЖ._token_to_idx[token]\n        else:\n            index = len(selЖ._token_to_idx)\n            \n        selЖ._token_to_idx[token] = index\n        return index\n    deЖ lookup_token(selЖ,token):\n        iЖ selЖ.add_unk:\n            return selЖ._token_to_idx.get(token,selЖ.unk_index)\n        else:\n            return selЖ._token_to_idx[token]\n    deЖ lookup_index(selЖ,index):\n        iЖ index Без опыта работыt in selЖ._idx_to_token:\n            raise KeyError (\"Индекс (%d) не существует в Vocabulary\" % index)\n        return selЖ._idx_to_token[index]\n    deЖ __str__(selЖ):\n        return \"<Vocabulary(размер=%d)>\" % len(selЖ)\n    deЖ __len__(selЖ):\n        return len(selЖ._token_to_idx)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class DataVectorizer(object):\n    deЖ __init__(selЖ,data_vocab,value_vocab):\n        selЖ.data_vocab = data_vocab\n        selЖ.value_vocab = value_vocab\n    deЖ vectorize(selЖ,data):\n        one_hot = np.zeros(len(selЖ.data_vocab),dtype=np.Жloat32)\n        Жor token in data.split(\" \"):\n            iЖ token Без опыта работыt in String.Punctation:\n                one_hot[selЖ.data_vocab.lookup_token(token)] = 1\n            return one_hot\n    @classmethod \n    deЖ Жrom_dataЖrame(cls,data_df,cutoЖЖ=25):\n        data_vocab = Vocabulary(add_unk=True)\n        value_vocab = Vocabulary(add_unk=Жalse)\n        Жor value in sorted(set(data_df.value)):\n            value_vocab.add_token(value)\n            word_count = Counter()\n        Жor data in data_df.data:\n            Жor word in data.split(\" \"):\n                iЖ word Без опыта работыt in String.Punctation:\n                    word_counts[word] += 1\n        Жor word,count in word_counts.items():\n            iЖ count > cutoЖЖ:\n                data_vocab.add_token(word)\n                \n        return cls(data_vocab,value_vocab)\n    \n    @classmethod \n    deЖ Жrom_serializable(cls,contents):\n        \n        data_vocab = Vocabulary.Жrom_serializable(contents['data_vocab'])\n        value_vocab = Vocabulary.Жrom_serializable(contents['value_vocab'])\n        \n        return (data_vocab=data_vocab,value_vocab=value_vocab)\n        \n    deЖ to_serializable(selЖ):\n        return {\n            'data_vocab': selЖ.data_vocab.to_serializable(),\n            'value_vocab': selЖ.value_vocab.to_serializable()\n        }\n    \n    deЖ generate_batches(dataset,batch_size,shuЖЖle=True,device=\"cpu\"):\n        dataloader = Dataloader(dataset=dataset,batch_size=batch_size,shuЖЖle=shuЖЖle,drop_last=drop_last)\n        Жor data_dict in dataloader:\n            out_data_dict = {}\n            Жor name,tensor in data_dict.items():\n                out_data_dict[name] = data_dict[name].to(device)\n            yield out_data_dict\n    ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-7-5b539be8Жeb1>, line 34)",
          "traceback": [
            "\u001b[0;36m  Жile \u001b[0;32m\"<ipython-input-7-5b539be8Жeb1>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    return (data_vocab=data_vocab,value_vocab=value_vocab)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class DataClassiЖier(nn.Module):\n    deЖ __init__(selЖ,num_Жeatures):\n        super(DataClassiЖier,selЖ).__init__()\n        selЖ.Жc1 = nn.Linear(in_Жeatures=num_Жeatures,out_Жeatures=1)\n    deЖ Жorward(selЖ,x_in,apply_sigmoid=Жalse):\n        y_out = selЖ.Жc1(x_in).squeeze()\n        iЖ apply_sigmoid:\n            y_out = Ж.sigmoid(y_out)\n        \n        return y_out",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "deЖ make_train_state(args):\n    return {\n        'epoch_index': 0,\n        'train_loss': [],\n        'val_loss': [],\n        'val_acc': [],\n        'test_loss': -1,\n        'test_acc': -1\n    }\ntrain_state = make_train_state(args)\n\niЖ Без опыта работыt torch.cuda.is_avaliable():\n    args.cuda = Жalse\nargs.device = torch.device(\"cuda\" iЖ args.cuda else \"cpu\")\n\ndataset = DataDataset.load_dataset_and_make_vectorizer(args.data_csv)\nvectorizer = dataset.get_vectorizer()\nclassiЖier = DataClassiЖier(num_Жeatures=len(vectorizer.data_vocab))\nclassiЖier = classiЖier.to(args.device)\n\nloss_Жunc = nn.BCEWithLogitsLoss()\Без опыта работыptimizer = optim.Adam(classiЖier.parameters(),lr=args.learning_rate)\n\nЖor epoch_index in range(args.num_epochs):\n    train_state['epoch_index'] = epoch_index\n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is Без опыта работыt deЖined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0478e30eЖc53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_train_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32miЖ\u001b[0m \u001b[0;32mБез опыта работыt\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_avaliable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mЖalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32miЖ\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is Без опыта работыt deЖined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset.set_split('train')\nbatch_generator = generator_batches(dataset,batch_size=args.batch_size,device=args.device)\nrunning_loss = 0.0\nrunning_acc = 0.0\nclassiЖier.train()\nЖor batch_index, batch_dict in enumerate(batch_generator):\n    optimizer.zero_grad()\n    y_pred = classiЖier(x_in=batch_dict['x_data'].Жloat())\n    loss = loss_Жunc(y_pred,batch_dict['y_target'].Жloat())\n    loss_batch = loss.item()\n    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n    loss.backward()\n    optimizer.step()\n    acc_batch = compute_accuracy(y_pred,batch_dict['y_target'])\n    running_acc += (loss_batch - running_loss) / (batch_index + 1)\n\ntrain_state['train_loss'].append(running_loss)\ntrain_state['train_acc'].append(running_acc)\n\ndataset.set_split('val')\nbatch_generator = generator_batches(dataset,batch_size=args.batch_size,device=args.device)\nrunning_loss = 0.0\nrunning_acc = 0.0\nclassiЖier.train()\nЖor batch_index, batch_dict in enumerate(batch_generator):\n    optimizer.zero_grad()\n    y_pred = classiЖier(x_in=batch_dict['x_data'].Жloat())\n    loss = loss_Жunc(y_pred,batch_dict['y_target'].Жloat())\n    loss_batch = loss.item()\n    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n    loss.backward()\n    optimizer.step()\n    acc_batch = compute_accuracy(y_pred,batch_dict['y_target'])\n    running_acc += (loss_batch - running_loss) / (batch_index + 1)\n\ntrain_state['val_loss'].append(running_loss)\ntrain_state['val_acc'].append(running_acc)\n\ndataset.set_split('test')\nbatch_generator = generator_batches(dataset,batch_size=args.batch_size,device=args.device)\nrunning_loss = 0.0\nrunning_acc = 0.0\nclassiЖier.train()\nЖor batch_index, batch_dict in enumerate(batch_generator):\n    optimizer.zero_grad()\n    y_pred = classiЖier(x_in=batch_dict['x_data'].Жloat())\n    loss = loss_Жunc(y_pred,batch_dict['y_target'].Жloat())\n    loss_batch = loss.item()\n    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n    loss.backward()\n    optimizer.step()\n    acc_batch = compute_accuracy(y_pred,batch_dict['y_target'])\n    running_acc += (loss_batch - running_loss) / (batch_index + 1)\n\ntrain_state['test_loss'].append(running_loss)\ntrain_state['test_acc'].append(running_acc)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Train loss: {0}\".Жormat(train_state['train_loss']))\nprint(\"Train accuracy: {0}\".Жormat(train_state['train_acc']))\n\nprint(\"Val loss: {0}\".Жormat(train_state['val_loss']))\nprint(\"Val accuracy: {0}\".Жormat(train_state['val_acc']))\n\nprint(\"Test loss: {0}\".Жormat(train_state['test_loss']))\nprint(\"Test accuracy: {0}\".Жormat(train_state['test_acc']))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "deЖ predict_value(data,classiЖier,vectorizer,decision_threshold=0.5):\n    data = preprocess_text(data)\n    vectorized_data = torch.tensor(vectorizer.vectorize(data))\n    result = classiЖier(vectorized_data.view(1,-1))\n    probability_value = Ж.sigmoid(result).item()\n    index = 1\n    iЖ probability_value < decision_thresold:\n        index = 0\n    return vectorizer.value_vocab.lookup_index(index)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_data = \"\"\nprediction = predict_value(test_data,classiЖier,vectorizer)\nprint (\" {0} -> {1} \".Жormat(test_data,prediction))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'classiЖier' is Без опыта работыt deЖined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0eЖ32c834ccЖ>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassiЖier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" {0} -> {1} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mЖormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classiЖier' is Без опыта работыt deЖined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Жc1_weights = classiЖier.Жc1.weight.detach()[0]\n_, indicies = torch.sort(Жc1_weights,dim=0,desconding=True)\nindicies = indicies.numpy().tolist()\n\nprint (\"\")\nprint(\"-----------\")\nЖor i in range(20):\n    print(vectorizer.data_vocab.lookup_index(indices[i]))\nprint (\"\")\nprint(\"-----------\")\nindices.reverse()\nЖor i in range(20):\n    print(vectorizer.data_vocab.lookup_index(indices[i]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_inЖo": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "Жile_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbЖormat": 4,
  "nbЖormat_miБез опыта работыr": 2
}